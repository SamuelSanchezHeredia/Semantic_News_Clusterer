# üóûÔ∏è Semantic News Clusterer

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![BERT](https://img.shields.io/badge/BERT-Sentence_Transformers-orange.svg)](https://www.sbert.net/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red.svg)](https://streamlit.io/)

## üìñ Descripci√≥n

**Semantic News Clusterer** es un sistema avanzado de clustering sem√°ntico que agrupa autom√°ticamente noticias similares utilizando t√©cnicas de NLP de √∫ltima generaci√≥n. A diferencia de los m√©todos tradicionales, este proyecto **no requiere etiquetas previas** y agrupa los textos bas√°ndose en su **significado sem√°ntico**, no solo en palabras clave.

### üéØ Objetivo

Desarrollar un pipeline completo para:
- üîç Descubrir autom√°ticamente temas en grandes vol√∫menes de noticias
- üìä Identificar patrones y tendencias sin supervisi√≥n humana
- üé® Visualizar clusters de manera interactiva
- üìù Interpretar y entender qu√© representa cada cluster
- üöÄ Aplicaci√≥n web interactiva con Streamlit para clasificar noticias en tiempo real

---

## üìë Tabla de Contenidos

1. [Stack Tecnol√≥gico](#-stack-tecnol√≥gico)
2. [Inicio R√°pido](#-inicio-r√°pido)
3. [Instalaci√≥n Completa](#-instalaci√≥n-completa)
4. [Aplicaci√≥n Streamlit](#-aplicaci√≥n-streamlit)
5. [Notebook Jupyter](#-notebook-jupyter)
6. [Integraci√≥n del Modelo](#-integraci√≥n-del-modelo-real)
7. [Dataset](#-dataset)
8. [Metodolog√≠a](#-metodolog√≠a)
9. [Configuraci√≥n](#-configuraci√≥n-personalizada)
10. [Soluci√≥n de Problemas](#-soluci√≥n-de-problemas)
11. [Referencias](#-referencias-y-recursos)

---

## üõ†Ô∏è Stack Tecnol√≥gico

### Componentes Principales

| Tecnolog√≠a | Prop√≥sito | Versi√≥n |
|------------|-----------|---------|
| **BERT** (Sentence Transformers) | Embeddings sem√°nticos | 5.2.2+ |
| **UMAP** | Reducci√≥n de dimensionalidad | 0.5.11+ |
| **HDBSCAN** | Clustering jer√°rquico | 0.8.41+ |
| **Plotly** | Visualizaciones interactivas | 6.5.2+ |
| **Streamlit** | Aplicaci√≥n web interactiva | 1.31.0+ |
| **Pandas** | Manipulaci√≥n de datos | 2.3.3+ |
| **Jupyter** | Entorno interactivo | 1.1.1+ |

### Pipeline del Sistema

```
üì• Descarga ‚Üí üßπ Limpieza ‚Üí ü§ñ BERT (384D) ‚Üí üîΩ UMAP (5D) ‚Üí üîç HDBSCAN ‚Üí üìä Visualizaci√≥n ‚Üí üìù Interpretaci√≥n
```

---

## ‚ö° Inicio R√°pido

### 1Ô∏è‚É£ Activar el entorno virtual

```bash
cd Semantic_News_Clusterer
source .venv/bin/activate  # En macOS/Linux
# .venv\Scripts\activate   # En Windows
```

### 2Ô∏è‚É£ Elegir tu m√©todo

#### Opci√≥n A: üåê Aplicaci√≥n Web Streamlit (Recomendado)

```bash
# Usar el script unificado
./run.sh streamlit

# O manualmente
streamlit run app_streamlit.py
```

**Caracter√≠sticas:**
- üìù Clasificar noticias individuales en tiempo real
- üìä Explorar clusters tem√°ticos
- üî¨ An√°lisis por lotes (m√∫ltiples noticias)
- üé® Visualizaciones interactivas

**Acceder en:** http://localhost:8501

#### Opci√≥n B: üìì Jupyter Notebook (An√°lisis completo)

```bash
# Usar el script unificado
./run.sh notebook

# O manualmente
jupyter notebook
```

Abre **`clustering_noticias_bert_hdbscan.ipynb`** y ejecuta:
- **Cell ‚Üí Run All** (para ejecutar todo)
- O **Shift + Enter** celda por celda

### 3Ô∏è‚É£ Flujo Completo (Entrenar + Streamlit)

```bash
# Script unificado que hace todo
./run.sh full
```

Este comando:
1. Verifica dependencias
2. Detecta si existe modelo entrenado
3. Te pregunta si quieres entrenar o usar existente
4. Ejecuta Streamlit con el modelo

‚è±Ô∏è **Tiempo estimado**: 
- App Streamlit: ~2 minutos (inicio)
- Notebook completo: ~10-15 minutos con 10,000 noticias

---

## üöÄ Instalaci√≥n Completa

### Pre-requisitos

- Python 3.8 o superior
- 8GB de RAM m√≠nimo
- Conexi√≥n a Internet (para descargar datos y modelos)

### Instalaci√≥n desde cero

```bash
# 1. Clonar o descargar el repositorio
cd Semantic_News_Clusterer

# 2. Crear entorno virtual
python3 -m venv .venv

# 3. Activar entorno virtual
source .venv/bin/activate  # macOS/Linux
# .venv\Scripts\activate   # Windows

# 4. Actualizar pip
pip install --upgrade pip

# 5. Instalar dependencias
pip install -r requirements.txt

# 6. Verificar instalaci√≥n
python verificar_entorno.py

# 7. Ejecutar la app
./run.sh streamlit
```

---

## üåê Aplicaci√≥n Streamlit

### üéØ Caracter√≠sticas

La aplicaci√≥n web incluye 3 tabs principales:

#### Tab 1: üìù Clasificar Noticia Individual
- Formulario para introducir titular y descripci√≥n
- Botones de ejemplo r√°pido (Pol√≠tica, Entretenimiento, Tecnolog√≠a)
- Muestra:
  - Cluster asignado con nombre descriptivo
  - Porcentaje de confianza
  - T√©rminos clave detectados
  - M√©tricas (palabras, cluster ID)
- Configuraci√≥n avanzada (ver embeddings, preprocesamiento)

#### Tab 2: üìä Explorar Clusters
- Visualiza todos los clusters tem√°ticos disponibles
- Gr√°fico de distribuci√≥n interactivo
- Detalles expandibles por cluster
- Informaci√≥n del modelo (si est√° entrenado)

#### Tab 3: üî¨ An√°lisis por Lotes
- **Opci√≥n 1**: Pegar m√∫ltiples noticias (formato: `Titular | Descripci√≥n`)
- **Opci√≥n 2**: Cargar archivo CSV
- Procesa todas las noticias a la vez
- Muestra tabla de resultados
- Gr√°fico de distribuci√≥n (pie chart)
- Bot√≥n para descargar resultados en CSV

### üöÄ Ejecutar la Aplicaci√≥n

```bash
# M√©todo 1: Script unificado
./run.sh streamlit

# M√©todo 2: Directamente
streamlit run app_streamlit.py

# Acceder en: http://localhost:8501
```

### üí° Ejemplos de Uso

#### Ejemplo 1: Noticia Pol√≠tica
```
Titular: Biden announces new climate policy
Descripci√≥n: The president unveiled sweeping climate change initiatives
```

#### Ejemplo 2: Noticia Tecnol√≥gica
```
Titular: Apple unveils new iPhone with AI features
Descripci√≥n: The tech giant announced revolutionary artificial intelligence capabilities
```

#### Ejemplo 3: An√°lisis Batch
Pega esto en el Tab 3:
```
Trump announces policy | New immigration rules
Apple releases iPhone | New AI features included
Lakers win championship | Basketball team claims title
```

### üõ†Ô∏è Tecnolog√≠as de la App

- **Streamlit**: Framework de aplicaci√≥n web
- **BERT** (all-MiniLM-L6-v2): Modelo de embeddings sem√°nticos
- **scikit-learn**: C√°lculo de similitud de coseno
- **Plotly**: Visualizaciones interactivas
- **Pandas/NumPy**: Procesamiento de datos

---

## üìì Notebook Jupyter

### üìñ Estructura del Notebook

El notebook est√° organizado en **11 secciones**:

1. **Importaci√≥n de Librer√≠as**: Setup inicial
2. **Carga de Datos**: Descarga desde Kaggle
3. **Preparaci√≥n**: Concatenaci√≥n de campos
4. **Preprocesamiento**: Limpieza de texto
5. **Embeddings BERT**: Vectorizaci√≥n sem√°ntica
6. **Reducci√≥n UMAP**: 5D para clustering, 2D para visualizaci√≥n
7. **Clustering HDBSCAN**: Identificaci√≥n autom√°tica de grupos
8. **Visualizaci√≥n**: Gr√°ficos interactivos
9. **Interpretaci√≥n**: Palabras clave y ejemplos
10. **An√°lisis de Calidad**: M√©tricas y evaluaci√≥n
11. **Exportaci√≥n**: Guardar modelo para Streamlit

### üöÄ Ejecutar el Notebook

```bash
# M√©todo 1: Script unificado
./run.sh notebook

# M√©todo 2: Directamente
jupyter notebook

# Abrir: clustering_noticias_bert_hdbscan.ipynb
# Ejecutar: Cell ‚Üí Run All
```

---

## üîó Integraci√≥n del Modelo Real

### ¬øC√≥mo funciona?

1. **Notebook** entrena el modelo con 10,000 noticias y guarda centroides en `model_data.pkl`
2. **Streamlit** carga autom√°ticamente el modelo real o usa demo si no existe

### üöÄ Usar el Modelo Real (3 Pasos)

#### Paso 1: Entrenar Modelo en el Notebook

```bash
./run.sh notebook
# O directamente: jupyter notebook
```

1. Abre `clustering_noticias_bert_hdbscan.ipynb`
2. **Ejecuta TODAS las celdas** (Cell ‚Üí Run All)
3. **Importante**: La √∫ltima secci√≥n guarda el modelo autom√°ticamente

**Ver√°s al final:**
```
üíæ GUARDANDO MODELO PARA LA APLICACI√ìN STREAMLIT
‚úì Centroides calculados para X clusters
‚úì Modelo guardado en: model_data.pkl
‚úì Tama√±o del archivo: XX KB

‚úÖ MODELO LISTO PARA USAR EN STREAMLIT
```

#### Paso 2: Verificar Archivo

```bash
ls -lh model_data.pkl
# Debe existir y tener tama√±o > 0
```

#### Paso 3: Ejecutar Streamlit

```bash
./run.sh streamlit
```

**Ver√°s en la app:**
```
‚úÖ Modelo REAL cargado correctamente

üìä Ver informaci√≥n del modelo entrenado ‚ñº
   Clusters: X
   Noticias entrenadas: 10,000
   Dimensi√≥n: 5D
```

### üîç Contenido de model_data.pkl

```python
{
    'centroids': {
        0: array([...]),  # Centroide del cluster 0 (384D - BERT original)
        1: array([...]),  # Centroide del cluster 1
        # ... m√°s clusters
    },
    'cluster_names': {
        0: "Trump & President (White)",
        1: "Movie & Film (Star)",
        # ... (nombres generados autom√°ticamente)
    },
    'model_name': 'all-MiniLM-L6-v2',
    'n_clusters': 6,
    'n_samples': 10000,
    'embedding_dimension': 384,  # BERT genera 384D
    'centroid_dimension': 384    # Centroides tambi√©n en 384D
}
```

### üéØ Caracter√≠sticas Modelo Real

| Aspecto | Modelo Real |
|---------|-------------|
| **Origen** | Entrenado con 10,000 noticias |
| **Centroides** | `mean(cluster_embeddings)` |
| **Clusters** | Los que HDBSCAN encontr√≥ |
| **Nombres** | Generados de t√©rminos reales |
| **Precisi√≥n**  | ‚úÖ Alta (~80%) |
| **Confianza** | >70% t√≠picamente |

---

## üìä Dataset

### News Category Dataset

- **Fuente**: [Kaggle - News Category Dataset](https://www.kaggle.com/datasets/rmisra/news-category-dataset)
- **Contenido**: ~200,000 noticias de HuffPost con categor√≠as
- **Formato**: JSON Lines
- **Descarga**: Autom√°tica mediante `kagglehub`
- **Campos principales**:
  - `headline`: T√≠tulo de la noticia
  - `short_description`: Descripci√≥n breve
  - `category`: Categor√≠a original (solo para validaci√≥n)

**El dataset se descarga autom√°ticamente al ejecutar el notebook.** No requiere configuraci√≥n manual.

---

## üî¨ Metodolog√≠a

### Pipeline Detallado

#### 1. **Carga de Datos**
- Descarga autom√°tica desde Kaggle
- Lectura de archivos JSON Lines
- Concatenaci√≥n de `headline` + `short_description`

#### 2. **Preprocesamiento**
```python
- Conversi√≥n a min√∫sculas
- Eliminaci√≥n de URLs y caracteres especiales
- Preservaci√≥n de estructura sem√°ntica para BERT
- Filtrado de textos muy cortos
```

#### 3. **Generaci√≥n de Embeddings**
```python
Modelo: 'all-MiniLM-L6-v2'
- Dimensiones: 384
- Velocidad: ~1000 textos/segundo
- Optimizado para similitud sem√°ntica
```

#### 4. **Reducci√≥n Dimensional (UMAP)**
```python
- De 384D ‚Üí 5D (para clustering)
- De 384D ‚Üí 2D (para visualizaci√≥n)
- Preserva estructura local y global
```

#### 5. **Clustering (HDBSCAN)**
```python
- Identifica clusters autom√°ticamente
- Separa outliers (ruido)
- Proporciona probabilidades de asignaci√≥n
```

#### 6. **Generaci√≥n de Nombres Descriptivos**
```python
- Extrae los 5 t√©rminos m√°s frecuentes por cluster
- Genera nombres autom√°ticos como "Trump & President (Election)"
- Filtra stopwords
```

#### 7. **Visualizaci√≥n e Interpretaci√≥n**
```python
- Gr√°ficos interactivos 2D (Plotly)
- Top 5 palabras clave por cluster
- T√≠tulos representativos
- An√°lisis de coherencia
```

---

## ‚öôÔ∏è Configuraci√≥n Personalizada

### Ajustar N√∫mero de Noticias

```python
# En el notebook:

# Prueba r√°pida (2 minutos)
df = load_news_dataset(sample_size=1000)

# An√°lisis medio (10-15 minutos) - RECOMENDADO
df = load_news_dataset(sample_size=10000)

# An√°lisis completo (~200k noticias, 1-2 horas)
df = load_news_dataset(sample_size=None)
```

### Ajustar Clustering (HDBSCAN)

#### M√°s clusters peque√±os (an√°lisis detallado):
```python
labels, clusterer = perform_clustering(
    embeddings_5d,
    min_cluster_size=30,   # ‚¨áÔ∏è Reducir
    min_samples=5          # ‚¨áÔ∏è Reducir
)
```

#### Menos clusters grandes (visi√≥n general):
```python
labels, clusterer = perform_clustering(
    embeddings_5d,
    min_cluster_size=100,  # ‚¨ÜÔ∏è Aumentar
    min_samples=20         # ‚¨ÜÔ∏è Aumentar
)
```

### Par√°metros UMAP

```python
# Estructura local (muchos clusters peque√±os)
embeddings_5d = reduce_dimensions(
    embeddings, n_components=5, n_neighbors=5
)

# Balance - RECOMENDADO
embeddings_5d = reduce_dimensions(
    embeddings, n_components=5, n_neighbors=15
)

# Estructura global (pocos clusters grandes)
embeddings_5d = reduce_dimensions(
    embeddings, n_components=5, n_neighbors=30
)
```

---

## üìà Resultados Esperados

### Con 10,000 noticias (configuraci√≥n por defecto)

| M√©trica | Valor T√≠pico |
|---------|--------------|
| **Clusters identificados** | 15-25 |
| **Cobertura** | 85-90% en clusters |
| **Ruido** | 10-15% |
| **Tiempo de ejecuci√≥n** | 10-15 minutos |
| **Confianza media** | >70% |

### M√©tricas de Calidad

| M√©trica | ‚úÖ Ideal | ‚ö†Ô∏è Aceptable | ‚ùå Problem√°tico |
|---------|----------|--------------|-----------------|
| **Clusters** | 15-30 | 10-50 | <5 o >100 |
| **Ruido %** | 5-15% | 15-25% | >30% |
| **Confianza** | >0.75 | 0.6-0.75 | <0.6 |

---

## üéõÔ∏è Script Unificado (run.sh)

El proyecto incluye un script unificado que maneja todas las operaciones:

```bash
# Ver ayuda
./run.sh help

# Ejecutar Streamlit
./run.sh streamlit

# Ejecutar Jupyter Notebook
./run.sh notebook

# Flujo completo (entrenar + streamlit)
./run.sh full

# Verificar instalaci√≥n
./run.sh check

# Limpiar archivos temporales
./run.sh clean
```

---

## üéì Casos de Uso

### üì∞ An√°lisis de Medios
- Identificar temas recurrentes en cobertura noticiosa
- Detectar sesgos informativos
- Comparar diferentes fuentes

### üìà Vigilancia de Tendencias
- Descubrir temas emergentes
- Monitorear evoluci√≥n de noticias
- Alertas de nuevos temas

### üóÇÔ∏è Organizaci√≥n de Contenido
- Agrupar art√≠culos similares
- Sistemas de recomendaci√≥n
- Deduplicaci√≥n de noticias

### üî¨ Research Acad√©mico
- An√°lisis de corpus de texto
- Estudios de comunicaci√≥n
- An√°lisis de sentimiento por cluster

---

## üìö Referencias y Recursos

### Documentaci√≥n T√©cnica

- [Sentence Transformers](https://www.sbert.net/) - Modelos BERT
- [UMAP Documentation](https://umap-learn.readthedocs.io/) - Reducci√≥n dimensional
- [HDBSCAN Guide](https://hdbscan.readthedocs.io/) - Clustering
- [Streamlit Docs](https://docs.streamlit.io/) - Framework web
- [News Dataset](https://www.kaggle.com/datasets/rmisra/news-category-dataset) - Datos

### Papers Relevantes

- Reimers & Gurevych (2019): "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
- McInnes et al. (2018): "UMAP: Uniform Manifold Approximation and Projection"
- Campello et al. (2013): "Density-Based Clustering Based on Hierarchical Density Estimates"

---

## üìÇ Estructura del Proyecto

```
Semantic_News_Clusterer/
‚îú‚îÄ‚îÄ üìì clustering_noticias_bert_hdbscan.ipynb  # Notebook principal
‚îú‚îÄ‚îÄ üåê app_streamlit.py                        # Aplicaci√≥n web
‚îú‚îÄ‚îÄ üîß run.sh                                  # Script unificado
‚îú‚îÄ‚îÄ ‚úÖ verificar_entorno.py                    # Verificador
‚îú‚îÄ‚îÄ üìã requirements.txt                        # Dependencias
‚îú‚îÄ‚îÄ üíæ model_data.pkl                          # Modelo entrenado (se genera)
‚îú‚îÄ‚îÄ üìÅ .venv/                                  # Entorno virtual
‚îî‚îÄ‚îÄ üìñ README.md                               # Este archivo
```

---

## üë§ Autor y Contribuciones

**Samuel Sanchez Heredia**

---

## üìÑ Licencia

Este proyecto est√° disponible bajo licencia MIT para uso educativo y de investigaci√≥n.

---

## ‚≠ê Si este proyecto te fue √∫til

- Dale una ‚≠ê en GitHub
- Comp√°rtelo con otros
- Contribuye con mejoras
- √ösalo como base para tus proyectos

---

<div align="center">

**üóûÔ∏è Semantic News Clusterer**

Desarrollado usando Python, BERT, UMAP, HDBSCAN y Streamlit

[![Made with Python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)
[![Made with Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?logo=Jupyter)](https://jupyter.org/)
[![Made with Streamlit](https://img.shields.io/badge/Made%20with-Streamlit-red?logo=Streamlit)](https://streamlit.io/)

---

**√öltima actualizaci√≥n**: Febrero 2026

**Versi√≥n**: 2.0

</div>

